\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1.1}{\ignorespaces caption for LOF}}{4}{figure.caption.4}%
\contentsline {figure}{\numberline {1.3.1}{\ignorespaces A unitary triangle (source: \cite {buras2002unitarity})\relax }}{6}{figure.caption.5}%
\contentsline {figure}{\numberline {1.3.2}{\ignorespaces Triangle constraints}}{6}{figure.caption.6}%
\contentsline {figure}{\numberline {1.4.1}{\ignorespaces Experiments}}{7}{figure.caption.7}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1.1}{\ignorespaces The entirety of the CERN accelerator complex. (source: \cite {VandenBroeck:2693837})\relax }}{9}{figure.caption.8}%
\contentsline {figure}{\numberline {2.2.1}{\ignorespaces A cross-section of the LHCb spectrometer in runs 1 and 2, the Velo detector, is visible in the far left of the image.\relax }}{10}{figure.caption.9}%
\contentsline {figure}{\numberline {2.3.1}{\ignorespaces Different media used in RICH with different Cherenkov angles versus their momentum.\relax }}{11}{figure.caption.10}%
\contentsline {figure}{\numberline {2.3.3}{\ignorespaces something}}{12}{figure.caption.12}%
\contentsline {figure}{\numberline {2.3.4}{\ignorespaces somethingelse}}{13}{figure.caption.13}%
\contentsline {figure}{\numberline {2.4.2}{\ignorespaces A depiction of Velo sensors and their routing lines. This is just the schematic representation as it does not reflect the actual number of strips and routing lines. The Phi sensors' outermost strips are directly connected to the readout chips without a need for long routing lines, but the innermost part has routing lanes lying parallel to the outermost strips. The R sensor routing is divided into sections of routes with different lengths. In reality, both sensors are divided into 512 long sections of repeated routing lines lengths. \relax }}{15}{figure.caption.15}%
\contentsline {figure}{\numberline {2.4.3}{\ignorespaces A detailed view of the Velo sensors with their according pitch. The R sensor's pitch starts at 38${"μ}m$ and ends on 102${"μ}m$. The Phi sensors' pitch modulation is divided in two. The most inner 683 strips have from 38 ${"μ}m$ to 78 ${"μ}m$ pitch, and the outer rest has a pitch ranging from 39 ${"μ}m$ to 97 ${"μ}m$. \relax }}{16}{figure.caption.16}%
\contentsline {figure}{\numberline {2.4.4}{\ignorespaces thing}}{17}{figure.caption.17}%
\contentsline {figure}{\numberline {2.4.5}{\ignorespaces Visualisation of the clustering algorithm. Two examples of Velo clusters are shown schematically. Blue rectangles depict the charges collected on the strips. Black lines represent high threshold $H_{t}$ and low threshold $L_{t}$ cuts, the reconstructed cluster centre (CC) is indicated by arrows and the particle trajectory is given by the red sloping lines.\relax }}{18}{figure.caption.18}%
\contentsline {figure}{\numberline {2.4.7}{\ignorespaces The projected data rate for each ASIC in Velo module (in Gbit/s).\relax }}{19}{figure.caption.20}%
\contentsline {figure}{\numberline {2.4.8}{\ignorespaces \relax }}{20}{figure.caption.21}%
\contentsline {figure}{\numberline {2.4.9}{\ignorespaces Exemplary threshold spread of the pixel matrix.\relax }}{21}{figure.caption.22}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.2.1}{\ignorespaces neron}}{24}{figure.caption.23}%
\contentsline {figure}{\numberline {3.2.2}{\ignorespaces singleann}}{25}{figure.caption.24}%
\contentsline {figure}{\numberline {3.2.3}{\ignorespaces multilayer ann}}{26}{figure.caption.25}%
\contentsline {figure}{\numberline {3.2.4}{\ignorespaces Step of application of a convolutional filter in a neural network \footnotemark \relax }}{29}{figure.caption.27}%
\contentsline {figure}{\numberline {3.2.5}{\ignorespaces Example of max pooling and average pooling\footnotemark . \relax }}{31}{figure.caption.28}%
\contentsline {figure}{\numberline {3.2.6}{\ignorespaces Graphical representation of the VGG16 architecture.\relax }}{32}{figure.caption.29}%
\contentsline {figure}{\numberline {3.3.1}{\ignorespaces autoenc}}{32}{figure.caption.30}%
\contentsline {figure}{\numberline {3.4.1}{\ignorespaces A representation of a simple neural network. The left representation is the rolled representation, and the right is a representation unrolled in the time dimension.\relax }}{33}{figure.caption.31}%
\contentsline {figure}{\numberline {3.4.2}{\ignorespaces recu}}{34}{figure.caption.32}%
\contentsline {figure}{\numberline {3.4.3}{\ignorespaces lstm}}{35}{figure.caption.33}%
\contentsline {figure}{\numberline {3.4.4}{\ignorespaces wei}}{36}{figure.caption.34}%
\contentsline {figure}{\numberline {3.5.1}{\ignorespaces rlenv}}{37}{figure.caption.35}%
\contentsline {figure}{\numberline {3.6.1}{\ignorespaces Exemplary steps of Metropolis-Hastings steps. In this example, the target distribution is $\mathcal {N}({"μ}=0, {"σ}=3)$. The accepted values are in blue, and the red crosses are rejects.\relax }}{42}{figure.caption.37}%
\contentsline {figure}{\numberline {3.7.1}{\ignorespaces Performance of the k-means algorithm on synthetic datasets. The upper left plot shows the effect of the algorithm using k=2, but the data clearly has 3 clusters. The right upper plot shows how the k-means fails to work with anisotropically distributed data, and the lower-left plot shows how it performs with the data of unequal variance. The last lower right plot shows an ideal case of a k-means algorithm with a proper number of clusters, with equal variance and isotropically distributed data. \relax }}{44}{figure.caption.38}%
\contentsline {figure}{\numberline {3.7.2}{\ignorespaces dbscan}}{44}{figure.caption.39}%
\contentsline {figure}{\numberline {3.7.3}{\ignorespaces Reachability plot (above) of the exemplary dataset (row below). The reachability plot colouring was created for a ${"ξ}$ \cite {10.1145/304182.304187} scheme of clustering. The first scatter plot in the second row represents clustering for the Xi scheme as well. The next two plots are for epsilon cuts as specified in the plot's titles. (source: scikit-learn \cite {scikit-learn})\relax }}{46}{figure.caption.41}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1.1}{\ignorespaces The schedule of the runs from 2019. This schedule has already been changed for 2020 due to the global pandemic.\relax }}{48}{figure.caption.42}%
\contentsline {figure}{\numberline {4.1.2}{\ignorespaces Histogram of the all of the pedestal values across all of the calibrations The histogram above is $P_{Ch, R, \#, T}$ and the below $P(Ch, {"φ}, \#, T)$).\relax }}{49}{figure.caption.44}%
\contentsline {figure}{\numberline {4.1.3}{\ignorespaces Pedestal plot for sensors (from the top) $P(Ch, R, \#64, T)$ $P(Ch, R, \#35, T)$ $P(Ch, R, \#85, T)$. $\#64$ and \#35 represent a typical pedestal values, while in \#85 there is a malfunction visible close to channel 1500.\relax }}{50}{figure.caption.45}%
\contentsline {figure}{\numberline {4.1.4}{\ignorespaces Histogram of linear coefficients of the pedestal trend per channel. Some of the outlying coefficient values have been trimmed and are outside the scope of this plot.\relax }}{51}{figure.caption.46}%
\contentsline {figure}{\numberline {4.1.5}{\ignorespaces High threshold distribution of all of the calibrations, across all sensors. $H_t(Ch,{"φ},\#, D)$ is above, and $H_t(Ch,R,\#, D)$ is the plot below.\relax }}{52}{figure.caption.47}%
\contentsline {figure}{\numberline {4.1.6}{\ignorespaces High threshold distribution with header cross-talk. $H_t(Ch, {"φ}, \#, D)$ above and $H_t(Ch, R, \#, D)$ below.\relax }}{52}{figure.caption.48}%
\contentsline {figure}{\numberline {4.1.7}{\ignorespaces High threshold distribution without header cross-talk. $H_t(Ch*, {"φ}, \#, D)$ above and $H_t(Ch*, R, \#, D)$ below.\relax }}{52}{figure.caption.49}%
\contentsline {figure}{\numberline {4.1.8}{\ignorespaces Histograms of imperfect calibrations in R sensors with their outlierness value $X$.\relax }}{53}{figure.caption.50}%
\contentsline {figure}{\numberline {4.1.9}{\ignorespaces Histograms of imperfect calibrations in {"φ}sensors with their outlierness value $X$.\relax }}{54}{figure.caption.51}%
\contentsline {figure}{\numberline {4.1.10}{\ignorespaces All calond}}{54}{figure.caption.52}%
\contentsline {figure}{\numberline {4.1.11}{\ignorespaces Distribution of blocked (masked) channels per sensor in time.\relax }}{55}{figure.caption.53}%
\contentsline {figure}{\numberline {4.1.12}{\ignorespaces The total number of occurrences of masked channels versus time.\relax }}{55}{figure.caption.54}%
\contentsline {figure}{\numberline {4.1.13}{\ignorespaces Anomalous values other than masked channels.\relax }}{56}{figure.caption.55}%
\contentsline {figure}{\numberline {4.2.1}{\ignorespaces 500 channels from a (a) single calibration date marked with X=10, (b) an example of a single generated calibration parameters at X=12, (c) an example of a single generated calibration parameters at X=12.\relax }}{60}{figure.caption.57}%
\contentsline {figure}{\numberline {4.2.2}{\ignorespaces Screenshot from the outlierness monitoring in Lovell software.\relax }}{60}{figure.caption.58}%
\contentsline {figure}{\numberline {4.3.1}{\ignorespaces \relax }}{61}{figure.caption.59}%
\contentsline {figure}{\numberline {4.3.2}{\ignorespaces All calibrationd}}{62}{figure.caption.60}%
\contentsline {figure}{\numberline {4.3.3}{\ignorespaces All calibrationd}}{63}{figure.caption.61}%
\contentsline {figure}{\numberline {4.3.4}{\ignorespaces All calond}}{64}{figure.caption.62}%
\contentsline {figure}{\numberline {4.3.5}{\ignorespaces Time progression of the selected section of calibration dates, with reduced dimentionality using PCA, only for $R$-type sensors.\relax }}{65}{figure.caption.63}%
\contentsline {figure}{\numberline {4.3.6}{\ignorespaces Time progression of the selected section of calibration dates, with reduced dimentionality using PCA, only for ${"Φ}$-type sensors.\relax }}{65}{figure.caption.64}%
\contentsline {figure}{\numberline {4.3.7}{\ignorespaces Time progression of the selected section of calibration dates, with reduced dimentionality using an autoencoder, only for R sensors.\relax }}{66}{figure.caption.65}%
\contentsline {figure}{\numberline {4.3.8}{\ignorespaces Time progression of the selected section of calibration dates, with reduced dimentionality using an autoencoder, only for phi sensors.\relax }}{66}{figure.caption.66}%
\contentsline {figure}{\numberline {4.3.9}{\ignorespaces All calibrationd}}{67}{figure.caption.67}%
\contentsline {figure}{\numberline {4.4.1}{\ignorespaces Two numerical solutions}}{69}{figure.caption.68}%
\contentsline {figure}{\numberline {4.4.2}{\ignorespaces The training process of the model.\relax }}{70}{figure.caption.70}%
\contentsline {figure}{\numberline {4.4.3}{\ignorespaces Real time to calibration (blue points) and the predicted by the trained WTTE-RNN model (red points)\relax }}{71}{figure.caption.71}%
\contentsline {figure}{\numberline {4.5.1}{\ignorespaces An example of clusterisation performed using DBSCAN (${"ε}$ = 10, MinPts = 4) on the binary pixel-map. Clusters are numbered 1-5.\relax }}{71}{figure.caption.72}%
\contentsline {figure}{\numberline {4.5.2}{\ignorespaces Rows represent clusters on image A in Figure \nobreakspace {}\ref {fig:fin_clus}, columns represent clusters on image B in Figure \nobreakspace {}\ref {fig:fin_clus}. Values indicate the Spacial Characteristics Similarity Measure \textit {V} (left plot), and Positional Similarity Measure \textit {${"Φ}$} (middle plot). The M matrix is the rightmost plot. \relax }}{74}{figure.caption.73}%
\contentsline {figure}{\numberline {4.5.3}{\ignorespaces The algorithm chooses clusters labelled with the same integer as the consecutive generations of the same cluster. Clusters labeled as 'new' are clusters on the time step $t_{n}$ that were absent on time step $t_{n-1}$.\relax }}{74}{figure.caption.74}%
\contentsline {figure}{\numberline {4.5.4}{\ignorespaces The fraction of pixels categorised as belonging to any cluster (Y-axis) in the next consecutive calibrations (X-axis), since the cluster introduction to calibration (number of timesteps $n=300$). The number of detected masked pixels slowly decreases with time. The OPTICS algorithm (left plot) recognises the masked pixels of the clusters as belonging to a cluster (not necessarily the same one) for a more extended amount of time. The DBSCAN is more strict in distinguishing the masked pixels that belong to clusters. \relax }}{75}{figure.caption.76}%
\contentsline {figure}{\numberline {4.5.5}{\ignorespaces The number of clusters classified as being "old" - same as in previous calibration (blue colour), and a number of clusters marked as "new" - not being the same clusters as in previous calibration (orange colour). The left plot belongs to OPTICS, and the one on the right to DBSCAN. \relax }}{76}{figure.caption.77}%
\contentsline {figure}{\numberline {4.6.1}{\ignorespaces A schematic of the TOT working mode.\relax }}{77}{figure.caption.78}%
\contentsline {figure}{\numberline {4.6.2}{\ignorespaces An exemplary plot of the surrogate function. Source: \cite {Platkevic:2013}.\relax }}{78}{figure.caption.79}%
\contentsline {figure}{\numberline {4.6.3}{\ignorespaces Surrogate parameters distribution part 1}}{79}{figure.caption.80}%
\contentsline {figure}{\numberline {4.6.4}{\ignorespaces Surrogate parameters distribution part2}}{80}{figure.caption.81}%
\contentsline {figure}{\numberline {4.6.5}{\ignorespaces A heat-map of the binning used in the analysis of the fluence. The binning starts from 0 at the centre, and the outermost bin is labelled as 7. Each colour represents a different elliptical binning.\relax }}{81}{figure.caption.82}%
\contentsline {figure}{\numberline {4.6.6}{\ignorespaces Parameters in s8 bins}}{82}{figure.caption.84}%
\contentsline {figure}{\numberline {4.6.7}{\ignorespaces Corelation surrogate params}}{83}{figure.caption.85}%
\contentsline {figure}{\numberline {4.6.8}{\ignorespaces Exemplary surrogates\relax }}{83}{figure.caption.86}%
\contentsline {figure}{\numberline {4.6.9}{\ignorespaces Exemplary results of the surrogate fluence model for the entire sensor. Each column of the plot represents a different parameter of the surrogate function. The first row of plot presents the distribution of respective surrogate model parameters as measured for the sensor S8. The middle row shows the distribution after scaling and applying the PCA, with the Gaussian function fit in orange. The bottom row shows the same actual distribution as in the first row in blue, and a distribution of generated parameters using the inverse model procedure. \relax }}{85}{figure.caption.87}%
\contentsline {figure}{\numberline {4.6.10}{\ignorespaces has to be capped}}{86}{figure.caption.88}%
\contentsline {figure}{\numberline {4.6.11}{\ignorespaces model pre ir}}{87}{figure.caption.89}%
\contentsline {figure}{\numberline {4.6.12}{\ignorespaces model post ir}}{88}{figure.caption.90}%
\contentsline {figure}{\numberline {4.6.13}{\ignorespaces Surrogate parameters based on fluence. The top row shows the distribution of the parameters in the pre irradiated sensor, and the bottom row presents the same distribution post-irradiation. Additionally, there is a third-degree polynomial curve fitted to this distribution.\relax }}{89}{figure.caption.91}%
\contentsline {figure}{\numberline {4.6.14}{\ignorespaces Distribution plots of the charge collected in bin 0. The left image shows the distribution made using the actual calibration of surrogates, and the right image shows the distribution made using generated surrogate parameters using our model.\relax }}{89}{figure.caption.92}%
\contentsline {figure}{\numberline {4.6.15}{\ignorespaces The plot of the MPV in different bins, using both the ground truth data and the generated parameters.\relax }}{90}{figure.caption.93}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1.1}{\ignorespaces The conceptual schematic of the data flow in the Storck service.\relax }}{93}{figure.caption.94}%
\contentsline {figure}{\numberline {5.1.2}{\ignorespaces Workspace endpoints of the REST API and their parameters (left) and the endpoints for interaction with Storck files (right). \relax }}{95}{figure.caption.96}%
\contentsline {figure}{\numberline {5.1.3}{\ignorespaces The schematic of the Storck deployment process. The light green colour means optional components.\relax }}{96}{figure.caption.97}%
\contentsline {figure}{\numberline {5.1.4}{\ignorespaces view of a GitLab pipeline\relax }}{96}{figure.caption.98}%
\contentsline {figure}{\numberline {5.1.5}{\ignorespaces Stocks web interface.\relax }}{97}{figure.caption.99}%
\contentsline {figure}{\numberline {5.2.1}{\ignorespaces Exemplary code combined into weather plot view in Titania.\relax }}{104}{figure.caption.101}%
\contentsline {figure}{\numberline {5.2.2}{\ignorespaces A screenshot of the exemplary titania web interface app \relax }}{107}{figure.caption.102}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.0.1}{\ignorespaces caption for LOF}}{110}{figure.caption.103}%
\contentsline {figure}{\numberline {6.0.2}{\ignorespaces A diagram of LARTPC planes \cite {microboone}\relax }}{110}{figure.caption.104}%
\contentsline {figure}{\numberline {6.1.1}{\ignorespaces Neutrino energy scale \cite {RevModPhys.84.1307}\relax }}{111}{figure.caption.105}%
\contentsline {figure}{\numberline {6.1.2}{\ignorespaces Beta decay}}{111}{figure.caption.106}%
\contentsline {figure}{\numberline {6.1.3}{\ignorespaces neutrino interactions}}{112}{figure.caption.107}%
\contentsline {figure}{\numberline {6.1.4}{\ignorespaces Beta decay}}{113}{figure.caption.108}%
\contentsline {figure}{\numberline {6.1.5}{\ignorespaces A diagram of deep inelastic interaction of a neutrino with a neutron (charged current)\cite {Conrad_1998}.\relax }}{113}{figure.caption.109}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {7.2.1}{\ignorespaces Exemplary data from the LARTPC dataset. \textbf {Left} image is the source input, and the \textbf {right} image is the desired output (target)\relax }}{117}{figure.caption.111}%
\contentsline {figure}{\numberline {7.2.2}{\ignorespaces Examples of zoom}}{117}{figure.caption.112}%
\contentsline {figure}{\numberline {7.4.1}{\ignorespaces Visualisation in the gym-lartpc package.\relax }}{122}{figure.caption.113}%
\contentsline {figure}{\numberline {7.5.1}{\ignorespaces The learning process of the baseline DDQN. \relax }}{123}{figure.caption.114}%
\contentsline {figure}{\numberline {7.5.2}{\ignorespaces Reinforcement learning setting for learning the DDQN algorithm.\relax }}{126}{figure.caption.115}%
\contentsline {figure}{\numberline {7.5.3}{\ignorespaces Exemplary input and output states for the neural network. The network receives both Source and Canvas input windows, and outputs a movement and put actions. \relax }}{127}{figure.caption.116}%
\contentsline {figure}{\numberline {7.5.4}{\ignorespaces The whole neural network model. Source and Canvas input windows enter the movement block after binarisation and concatenation. The Source input alone also enters the classification block as is. The movement block returns a vector of size N. N is the number of possible movement directions (usually N=8). The classification block returns a vectorised matrix of classified pixels, of length K. Usually K=1 or K=9. Both blocks outputs are then concatenated and passed to the joint layer, which outputs a vector of size N+K.\relax }}{128}{figure.caption.117}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.0.1}{\ignorespaces A plot explaining the scope of the boxplot (@TODO add this to appendix). The top plot represent the boxplot, with median in the middle. The range of the box spans from Quartile 1 to Quartile 3. The interquartile region (IQR) is the width of the box. The whiskers span from the edges of the box to length of 1.5 IQR to either side of the axis. The two bottom plots explain the ranges in terms of percentages and width of the distribution. Additionaly, an outlier is a value lying outside the both whiskers range. \relax }}{130}{figure.caption.120}%
